{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkpsIDEzo2JOjTYNpZrjJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sw6820/tensorflow_certificate/blob/main/tensorflow_certificate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wggJnyRYQV3x",
        "outputId": "f3d47452-25e6-44d2-aef8-f97720850871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# from tensorflow import keras"
      ],
      "metadata": {
        "id": "S1Un_SY8Rbr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prob1"
      ],
      "metadata": {
        "id": "DpFzb9GgfNaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb"
      ],
      "metadata": {
        "id": "UzTRoI1xwGTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test model button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# Getting Started Question\n",
        "#\n",
        "# Given this data, train a neural network to match the xs to the ys\n",
        "# So that a predictor for a new value of X will give a float value\n",
        "# very close to the desired answer\n",
        "# i.e. print(model.predict([10.0])) would give a satisfactory result\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1]\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def solution_model():\n",
        "    xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "    ys = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "    model.compile(optimizer='SGD', loss='mean_squared_error')\n",
        "    model.fit(xs, ys, epochs=1000)\n",
        "    return model\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "id": "Vc_goinzfM5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prob2"
      ],
      "metadata": {
        "id": "Ox7-_jErRSht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%204%20-%20Lesson%204%20-%20Notebook.ipynb"
      ],
      "metadata": {
        "id": "I93Y6qSGwTrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# Basic Datasets Question\n",
        "#\n",
        "# Create and train a classifier for the MNIST dataset.\n",
        "# Note that the test will expect it to classify 10 classes and that the \n",
        "# input shape should be the native size of the MNIST dataset which is \n",
        "# 28x28 monochrome. Do not resize the data. Your input layer should accept\n",
        "# (28,28) as the input shape only. If you amend this, the tests will fail.\n",
        "#\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.6):\n",
        "      print(\"\\nReached 60% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "def solution_model():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "    callbacks = myCallback()\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "    ])\n",
        "    model.compile(optimizer=tf.optimizers.Adam(),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])\n",
        "    # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tz3usR_QbGL",
        "outputId": "74dc4b46-a09e-4573-ed49-6ee5340949be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9413\n",
            "Reached 60% accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 7s 2ms/step - loss: 0.1982 - accuracy: 0.9413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prob3"
      ],
      "metadata": {
        "id": "ttOwJApoS_ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%208%20-%20Lesson%202%20-%20Notebook%20(RockPaperScissors).ipynb"
      ],
      "metadata": {
        "id": "ZjI1hdsWwi_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# Computer Vision with CNNs\n",
        "#\n",
        "# Build a classifier for Rock-Paper-Scissors based on the rock_paper_scissors\n",
        "# TensorFlow dataset.\n",
        "#\n",
        "# IMPORTANT: Your final layer should be as shown. Do not change the\n",
        "# provided code, or the tests may fail\n",
        "#\n",
        "# IMPORTANT: Images will be tested as 150x150 with 3 bytes of color depth\n",
        "# So ensure that your input layer is designed accordingly, or the tests\n",
        "# may fail. \n",
        "#\n",
        "# NOTE THAT THIS IS UNLABELLED DATA. \n",
        "# You can use the ImageDataGenerator to automatically label it\n",
        "# and we have provided some starter code.\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n",
        "    urllib.request.urlretrieve(url, 'rps.zip')\n",
        "    local_zip = 'rps.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/')\n",
        "    zip_ref.close()\n",
        "\n",
        "\n",
        "    TRAINING_DIR = \"tmp/rps/\"\n",
        "    training_datagen = ImageDataGenerator(\n",
        "    # YOUR CODE HERE)\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "    train_generator = training_datagen.flow_from_directory(\n",
        "        TRAINING_DIR,\n",
        "        target_size=(150,150),\n",
        "        class_mode='categorical',\n",
        "        batch_size=126\n",
        "    )# YOUR CODE HERE\n",
        "\n",
        "    VALIDATION_DIR = \"/tmp/\"\n",
        "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        VALIDATION_DIR,\n",
        "        target_size=(150, 150),\n",
        "        class_mode='categorical',\n",
        "        batch_size=126\n",
        "    )\n",
        "    model = tf.keras.models.Sequential([\n",
        "    # YOUR CODE HERE, BUT END WITH A 3 Neuron Dense, activated by softmax\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        # The second convolution\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        # The third convolution\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        # The fourth convolution\n",
        "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2,2),\n",
        "        # Flatten the results to feed into a DNN\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # 512 neuron hidden layer\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_generator, epochs=25, steps_per_epoch=20, validation_data=validation_generator, verbose=1,\n",
        "                        validation_steps=3)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbhimzggRaQN",
        "outputId": "1244d984-082e-47d7-b27d-1dc770d9cfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2520 images belonging to 3 classes.\n",
            "Found 0 images belonging to 5 classes.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 148, 148, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 74, 74, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 72, 72, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 36, 36, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  (None, 17, 17, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  (None, 7, 7, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,473,475\n",
            "Trainable params: 3,473,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "20/20 [==============================] - 20s 960ms/step - loss: 1.2806 - accuracy: 0.3333\n",
            "Epoch 2/25\n",
            "20/20 [==============================] - 19s 948ms/step - loss: 1.0949 - accuracy: 0.3683\n",
            "Epoch 3/25\n",
            "20/20 [==============================] - 19s 957ms/step - loss: 1.0721 - accuracy: 0.3940\n",
            "Epoch 4/25\n",
            "20/20 [==============================] - 19s 947ms/step - loss: 1.0760 - accuracy: 0.5175\n",
            "Epoch 5/25\n",
            "20/20 [==============================] - 19s 943ms/step - loss: 0.9624 - accuracy: 0.5512\n",
            "Epoch 6/25\n",
            "20/20 [==============================] - 19s 955ms/step - loss: 0.7563 - accuracy: 0.6504\n",
            "Epoch 7/25\n",
            "20/20 [==============================] - 19s 949ms/step - loss: 0.6644 - accuracy: 0.7179\n",
            "Epoch 8/25\n",
            "20/20 [==============================] - 19s 947ms/step - loss: 0.5235 - accuracy: 0.7861\n",
            "Epoch 9/25\n",
            "20/20 [==============================] - 19s 945ms/step - loss: 0.5230 - accuracy: 0.7956\n",
            "Epoch 10/25\n",
            "20/20 [==============================] - 19s 946ms/step - loss: 0.3414 - accuracy: 0.8587\n",
            "Epoch 11/25\n",
            "20/20 [==============================] - 19s 952ms/step - loss: 0.3530 - accuracy: 0.8766\n",
            "Epoch 12/25\n",
            "20/20 [==============================] - 19s 946ms/step - loss: 0.2707 - accuracy: 0.8948\n",
            "Epoch 13/25\n",
            "20/20 [==============================] - 19s 949ms/step - loss: 0.2684 - accuracy: 0.8937\n",
            "Epoch 14/25\n",
            "20/20 [==============================] - 19s 948ms/step - loss: 0.2706 - accuracy: 0.9187\n",
            "Epoch 15/25\n",
            "20/20 [==============================] - 19s 953ms/step - loss: 0.2551 - accuracy: 0.9079\n",
            "Epoch 16/25\n",
            "20/20 [==============================] - 19s 945ms/step - loss: 0.1328 - accuracy: 0.9544\n",
            "Epoch 17/25\n",
            "20/20 [==============================] - 19s 932ms/step - loss: 0.1890 - accuracy: 0.9242\n",
            "Epoch 18/25\n",
            "20/20 [==============================] - 19s 930ms/step - loss: 0.1117 - accuracy: 0.9639\n",
            "Epoch 19/25\n",
            "20/20 [==============================] - 19s 938ms/step - loss: 0.2037 - accuracy: 0.9321\n",
            "Epoch 20/25\n",
            "20/20 [==============================] - 19s 938ms/step - loss: 0.1303 - accuracy: 0.9567\n",
            "Epoch 21/25\n",
            "20/20 [==============================] - 19s 936ms/step - loss: 0.1180 - accuracy: 0.9575\n",
            "Epoch 22/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.1309 - accuracy: 0.9583\n",
            "Epoch 23/25\n",
            "20/20 [==============================] - 19s 931ms/step - loss: 0.1458 - accuracy: 0.9524\n",
            "Epoch 24/25\n",
            "20/20 [==============================] - 19s 939ms/step - loss: 0.1273 - accuracy: 0.9536\n",
            "Epoch 25/25\n",
            "20/20 [==============================] - 20s 973ms/step - loss: 0.0455 - accuracy: 0.9877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prob4"
      ],
      "metadata": {
        "id": "B13jkvG_W40z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202.ipynb"
      ],
      "metadata": {
        "id": "cy7annFMwq2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# Build and train a classifier for the sarcasm dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    urls = []\n",
        "    with open(\"/content/sarcasm.json\", 'r') as f:\n",
        "        datastore = json.load(f)\n",
        "\n",
        "    for item in datastore:\n",
        "        sentences.append(item['headline'])\n",
        "        labels.append(item['is_sarcastic'])\n",
        "        urls.append(item['article_link'])\n",
        "    training_sentences = sentences[0:training_size]\n",
        "    testing_sentences = sentences[training_size:]\n",
        "    training_labels = labels[0:training_size]\n",
        "    testing_labels = labels[training_size:]\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    \n",
        "\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    training_padded = np.array(training_padded)\n",
        "    training_labels = np.array(training_labels)\n",
        "    testing_padded = np.array(testing_padded)\n",
        "    testing_labels = np.array(testing_labels)\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),        \n",
        "        tf.keras.layers.Dropout(0.2),#\n",
        "        tf.keras.layers.Dense(32),#\n",
        "        tf.keras.layers.Dropout(0.2),#\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),#\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')        \n",
        "    ])\n",
        "    num_epochs=50\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5,\n",
        "                                         beta_1=0.9,\n",
        "                                         beta_2=0.999,\n",
        "                                         epsilon=1e-07,\n",
        "                                         ) ##\n",
        "    callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    min_delta=0.0001,\n",
        "    cooldown=0,\n",
        "    min_lr=0,\n",
        "    \n",
        "    )                                         \n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1, callbacks=[callback])\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blv1lQoGW6B5",
        "outputId": "0505cd75-66da-45da-87c1-9f239257a4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_18 (Embedding)    (None, 120, 16)           16000     \n",
            "                                                                 \n",
            " bidirectional_21 (Bidirecti  (None, 128)              41472     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 24)                792       \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 24)                0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,417\n",
            "Trainable params: 62,417\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5571"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f6ea7954320> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f6ea7954320> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "625/625 [==============================] - 11s 14ms/step - loss: 0.6866 - accuracy: 0.5572 - val_loss: 0.6794 - val_accuracy: 0.5633 - lr: 5.0000e-05\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.6230 - accuracy: 0.6489 - val_loss: 0.5233 - val_accuracy: 0.7564 - lr: 5.0000e-05\n",
            "Epoch 3/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.4858 - accuracy: 0.7773 - val_loss: 0.4527 - val_accuracy: 0.7964 - lr: 5.0000e-05\n",
            "Epoch 4/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.4386 - accuracy: 0.8059 - val_loss: 0.4291 - val_accuracy: 0.8097 - lr: 5.0000e-05\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.4110 - accuracy: 0.8173 - val_loss: 0.4130 - val_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3953 - accuracy: 0.8239 - val_loss: 0.4044 - val_accuracy: 0.8167 - lr: 5.0000e-05\n",
            "Epoch 7/50\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.3822 - accuracy: 0.8302 - val_loss: 0.3986 - val_accuracy: 0.8199 - lr: 5.0000e-05\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3746 - accuracy: 0.8349 - val_loss: 0.3945 - val_accuracy: 0.8204 - lr: 5.0000e-05\n",
            "Epoch 9/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3673 - accuracy: 0.8397 - val_loss: 0.3924 - val_accuracy: 0.8217 - lr: 5.0000e-05\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3612 - accuracy: 0.8414 - val_loss: 0.3905 - val_accuracy: 0.8223 - lr: 5.0000e-05\n",
            "Epoch 11/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3573 - accuracy: 0.8449 - val_loss: 0.3885 - val_accuracy: 0.8213 - lr: 5.0000e-05\n",
            "Epoch 12/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3524 - accuracy: 0.8454 - val_loss: 0.3872 - val_accuracy: 0.8237 - lr: 5.0000e-05\n",
            "Epoch 13/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3490 - accuracy: 0.8482 - val_loss: 0.3936 - val_accuracy: 0.8211 - lr: 5.0000e-05\n",
            "Epoch 14/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3456 - accuracy: 0.8513 - val_loss: 0.3882 - val_accuracy: 0.8244 - lr: 5.0000e-05\n",
            "Epoch 15/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3425 - accuracy: 0.8539 - val_loss: 0.3908 - val_accuracy: 0.8223 - lr: 5.0000e-05\n",
            "Epoch 16/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3389 - accuracy: 0.8540 - val_loss: 0.3898 - val_accuracy: 0.8232 - lr: 5.0000e-05\n",
            "Epoch 17/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3374 - accuracy: 0.8558 - val_loss: 0.3863 - val_accuracy: 0.8235 - lr: 5.0000e-05\n",
            "Epoch 18/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3326 - accuracy: 0.8575 - val_loss: 0.3901 - val_accuracy: 0.8249 - lr: 5.0000e-05\n",
            "Epoch 19/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3323 - accuracy: 0.8586 - val_loss: 0.3917 - val_accuracy: 0.8238 - lr: 5.0000e-05\n",
            "Epoch 20/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3298 - accuracy: 0.8612 - val_loss: 0.3851 - val_accuracy: 0.8271 - lr: 5.0000e-05\n",
            "Epoch 21/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3274 - accuracy: 0.8619 - val_loss: 0.3846 - val_accuracy: 0.8275 - lr: 5.0000e-05\n",
            "Epoch 22/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3268 - accuracy: 0.8632 - val_loss: 0.3846 - val_accuracy: 0.8255 - lr: 5.0000e-05\n",
            "Epoch 23/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3221 - accuracy: 0.8639 - val_loss: 0.3843 - val_accuracy: 0.8267 - lr: 5.0000e-05\n",
            "Epoch 24/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3185 - accuracy: 0.8657 - val_loss: 0.3864 - val_accuracy: 0.8264 - lr: 5.0000e-05\n",
            "Epoch 25/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3174 - accuracy: 0.8672 - val_loss: 0.3862 - val_accuracy: 0.8274 - lr: 5.0000e-05\n",
            "Epoch 26/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3163 - accuracy: 0.8674 - val_loss: 0.3838 - val_accuracy: 0.8283 - lr: 5.0000e-05\n",
            "Epoch 27/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3149 - accuracy: 0.8684 - val_loss: 0.3859 - val_accuracy: 0.8271 - lr: 5.0000e-05\n",
            "Epoch 28/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3118 - accuracy: 0.8699 - val_loss: 0.3846 - val_accuracy: 0.8281 - lr: 5.0000e-05\n",
            "Epoch 29/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3090 - accuracy: 0.8705 - val_loss: 0.3875 - val_accuracy: 0.8281 - lr: 5.0000e-05\n",
            "Epoch 30/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3090 - accuracy: 0.8726 - val_loss: 0.3857 - val_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 31/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3049 - accuracy: 0.8736 - val_loss: 0.3865 - val_accuracy: 0.8299 - lr: 5.0000e-05\n",
            "Epoch 32/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3053 - accuracy: 0.8742 - val_loss: 0.3888 - val_accuracy: 0.8292 - lr: 5.0000e-05\n",
            "Epoch 33/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3029 - accuracy: 0.8773 - val_loss: 0.3876 - val_accuracy: 0.8272 - lr: 5.0000e-05\n",
            "Epoch 34/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3007 - accuracy: 0.8763 - val_loss: 0.3886 - val_accuracy: 0.8295 - lr: 5.0000e-05\n",
            "Epoch 35/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2995 - accuracy: 0.8774 - val_loss: 0.3863 - val_accuracy: 0.8301 - lr: 5.0000e-05\n",
            "Epoch 36/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2977 - accuracy: 0.8791 - val_loss: 0.3907 - val_accuracy: 0.8293 - lr: 5.0000e-05\n",
            "Epoch 37/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2913 - accuracy: 0.8830 - val_loss: 0.3901 - val_accuracy: 0.8305 - lr: 5.0000e-06\n",
            "Epoch 38/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2929 - accuracy: 0.8823 - val_loss: 0.3906 - val_accuracy: 0.8307 - lr: 5.0000e-06\n",
            "Epoch 39/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2917 - accuracy: 0.8824 - val_loss: 0.3906 - val_accuracy: 0.8299 - lr: 5.0000e-06\n",
            "Epoch 40/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2919 - accuracy: 0.8825 - val_loss: 0.3907 - val_accuracy: 0.8301 - lr: 5.0000e-06\n",
            "Epoch 41/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2933 - accuracy: 0.8823 - val_loss: 0.3909 - val_accuracy: 0.8301 - lr: 5.0000e-06\n",
            "Epoch 42/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2913 - accuracy: 0.8823 - val_loss: 0.3913 - val_accuracy: 0.8305 - lr: 5.0000e-06\n",
            "Epoch 43/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2917 - accuracy: 0.8838 - val_loss: 0.3916 - val_accuracy: 0.8305 - lr: 5.0000e-06\n",
            "Epoch 44/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2918 - accuracy: 0.8835 - val_loss: 0.3919 - val_accuracy: 0.8308 - lr: 5.0000e-06\n",
            "Epoch 45/50\n",
            "625/625 [==============================] - 11s 18ms/step - loss: 0.2910 - accuracy: 0.8833 - val_loss: 0.3920 - val_accuracy: 0.8308 - lr: 5.0000e-06\n",
            "Epoch 46/50\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.2907 - accuracy: 0.8837 - val_loss: 0.3921 - val_accuracy: 0.8319 - lr: 5.0000e-06\n",
            "Epoch 47/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2912 - accuracy: 0.8839 - val_loss: 0.3922 - val_accuracy: 0.8313 - lr: 5.0000e-07\n",
            "Epoch 48/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2903 - accuracy: 0.8847 - val_loss: 0.3922 - val_accuracy: 0.8313 - lr: 5.0000e-07\n",
            "Epoch 49/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2903 - accuracy: 0.8843 - val_loss: 0.3922 - val_accuracy: 0.8316 - lr: 5.0000e-07\n",
            "Epoch 50/50\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2908 - accuracy: 0.8832 - val_loss: 0.3922 - val_accuracy: 0.8317 - lr: 5.0000e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be build. `model.compile_metrics` will be empty until you train or evaluate the model."
      ],
      "metadata": {
        "id": "rBqN1WUIentq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prob5"
      ],
      "metadata": {
        "id": "n7DyNCRZZbSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict time indexed variables of\n",
        "# the multivariate house hold electric power consumption time series dataset.\n",
        "# Using a window of past 24 observations of the 7 variables, the model\n",
        "# should be trained to predict the next 24 observations of the 7 variables.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption\n",
        "#\n",
        "# The original Individual House Hold Electric Power Consumption Dataset\n",
        "# has Measurements of electric power consumption in one household with\n",
        "# a one-minute sampling rate over a period of almost 4 years.\n",
        "#\n",
        "# Different electrical quantities and some sub-metering values are available.\n",
        "#\n",
        "# For the purpose of the examination we have provided a subset containing\n",
        "# the data for the first 60 days in the dataset. We have also cleaned the\n",
        "# dataset beforehand to remove missing values. The dataset is provided as a\n",
        "# CSV file in the project.\n",
        "#\n",
        "# The dataset has a total of 7 features ordered by time.\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 24\n",
        "#    observations of the 7 features to predict the next N_FUTURE = 24\n",
        "#    observations of the same features.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 7 neurons since\n",
        "#    the model is expected to predict observations of 7 features.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.055 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and\n",
        "# validation. First element of the first window will be the first element of\n",
        "# the dataset. Consecutive windows are constructed by shifting\n",
        "# the starting position of the first window forward, one at a time (indicated\n",
        "# by shift=1). For a window of n_past number of observations of all the time\n",
        "# indexed variables in the dataset, the target for the window\n",
        "# is the next n_future number of observations of these variables, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "\n",
        "# This function loads the data from CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "    # Reads the dataset from the CSV.\n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
        "                     infer_datetime_format=True, index_col='datetime', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features at future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.5) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 24  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 24  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "        # ADD YOUR LAYERS HERE.\n",
        "        tf.keras.layers.Conv1D(filters=64,\n",
        "                               kernel_size=5,\n",
        "                               strides=1,\n",
        "                               padding=\"causal\",\n",
        "                               activation='relu',\n",
        "                               input_shape=[None, N_FEATURES]),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(32),\n",
        "        tf.keras.layers.Dense(16),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # The input layer of your model must have an input shape of:\n",
        "        # (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)\n",
        "        # The model must have an output shape of:\n",
        "        # (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7).\n",
        "        # Make sure that there are N_FEATURES = 7 neurons in the final dense\n",
        "        # layer since the model predicts 7 features.\n",
        "\n",
        "        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
        "        # suggestion.\n",
        "\n",
        "        # WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "        # recurrent_dropout argument (you can alternatively set it to 0),\n",
        "        # since it has not been implemented in the cuDNN kernel and may\n",
        "        # result in much longer training times.\n",
        "        tf.keras.layers.Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    # Code to train and compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-4,\n",
        "                                         beta_1=0.9,\n",
        "                                         beta_2=0.999,\n",
        "                                         epsilon=1e-07,\n",
        "                                         ) # YOUR CODE HERE\n",
        "    model.compile(\n",
        "        # YOUR CODE HERE\n",
        "        loss=\"mae\",\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"mae\"]\n",
        "    )\n",
        "    model.fit(\n",
        "        # YOUR CODE HERE\n",
        "        train_set, epochs=10, validation_data=valid_set\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"c5q4.h5\")\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "#def mae(y_true, y_pred):\n",
        "#    return np.mean(abs(y_true.ravel() - y_pred.ravel()))\n",
        "#\n",
        "#\n",
        "#def model_forecast(model, series, window_size, batch_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "#\n",
        "\n",
        "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "\n",
        "#rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
        "#rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, :]\n",
        "\n",
        "#x_valid = x_valid[:rnn_forecast.shape[0]]\n",
        "#result = mae(x_valid, rnn_forecast)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5Rc08KAQjwh",
        "outputId": "2487173a-4b55-4bcf-ff7b-f3a075f2d1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "   1349/Unknown - 27s 15ms/step - loss: 0.0858 - mae: 0.0858"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f6ea7c21d40> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f6ea7c21d40> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1349/1349 [==============================] - 49s 32ms/step - loss: 0.0858 - mae: 0.0858 - val_loss: 0.0636 - val_mae: 0.0636\n",
            "Epoch 2/10\n",
            "1349/1349 [==============================] - 32s 24ms/step - loss: 0.0681 - mae: 0.0681 - val_loss: 0.0560 - val_mae: 0.0560\n",
            "Epoch 3/10\n",
            "1349/1349 [==============================] - 32s 24ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.0540 - val_mae: 0.0540\n",
            "Epoch 4/10\n",
            "1349/1349 [==============================] - 32s 24ms/step - loss: 0.0593 - mae: 0.0593 - val_loss: 0.0523 - val_mae: 0.0523\n",
            "Epoch 5/10\n",
            "1349/1349 [==============================] - 32s 23ms/step - loss: 0.0578 - mae: 0.0578 - val_loss: 0.0515 - val_mae: 0.0515\n",
            "Epoch 6/10\n",
            "1349/1349 [==============================] - 32s 24ms/step - loss: 0.0569 - mae: 0.0569 - val_loss: 0.0500 - val_mae: 0.0500\n",
            "Epoch 7/10\n",
            "1349/1349 [==============================] - 31s 23ms/step - loss: 0.0562 - mae: 0.0562 - val_loss: 0.0509 - val_mae: 0.0509\n",
            "Epoch 8/10\n",
            "1349/1349 [==============================] - 32s 24ms/step - loss: 0.0556 - mae: 0.0556 - val_loss: 0.0510 - val_mae: 0.0510\n",
            "Epoch 9/10\n",
            "1349/1349 [==============================] - 35s 26ms/step - loss: 0.0550 - mae: 0.0550 - val_loss: 0.0498 - val_mae: 0.0498\n",
            "Epoch 10/10\n",
            "1349/1349 [==============================] - 32s 24ms/step - loss: 0.0545 - mae: 0.0545 - val_loss: 0.0496 - val_mae: 0.0496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YfDEhZJFRRC-",
        "outputId": "363c8ff0-b59d-426a-aeee-101edf8fb5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     || 511.7 MB 4.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (1.48.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (1.1.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     || 1.6 MB 55.1 MB/s \n",
            "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (21.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (14.0.6)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     || 5.8 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (57.4.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     || 438 kB 75.3 MB/s \n",
            "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (4.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (0.26.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.0) (1.21.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.9.0) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow==2.9.0) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0.7\n",
            "    Uninstalling flatbuffers-2.0.7:\n",
            "      Successfully uninstalled flatbuffers-2.0.7\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flatbuffers",
                  "gast",
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 5/5\n",
        "2. 5/5\n",
        "3. 5/5\n",
        "4. 4/5\n",
        "5. 5/5\n"
      ],
      "metadata": {
        "id": "QSptjOKzvlGb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1qHidjpcsQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}